{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, string, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation, Embedding, Dropout, TimeDistributed\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras import metrics\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from numpy.random import seed\n",
    "seed(9)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"student\", \"faculty\", \"staff\", \"department\", \"course\", \"project\", \"other\"]\n",
    "class_index = dict((c, i) for i, c in enumerate(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'student': 0, 'faculty': 1, 'staff': 2, 'department': 3, 'course': 4, 'project': 5, 'other': 6}\n"
     ]
    }
   ],
   "source": [
    "print(class_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create list of all files, along with their classes\n",
    "all_files varia\n",
    "all_files : {\"student\": [[\"file_path\"],[\"file_path\"]], \"course\": [[\"file_path2\", \"file_path3\"]]}\n",
    "\"\"\"\n",
    "all_files = {}\n",
    "path = \"data/raw/webkb/\"\n",
    "all_folders = os.listdir(path)\n",
    "for clz in all_folders:\n",
    "    if clz.startswith('.'):\n",
    "        continue\n",
    "    if clz not in all_files:\n",
    "        all_files[clz] = []\n",
    "    path_with_clz = path + clz + '/'\n",
    "    all_univs = os.listdir(path_with_clz)\n",
    "    for univ in all_univs:\n",
    "        if univ.startswith('.'):\n",
    "            continue\n",
    "        path_with_univs = path_with_clz + univ + '/'\n",
    "        all_files[clz].append(glob.glob(os.path.join(path_with_univs, '*')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(all_files[\"department\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['faculty', 'course', 'other', 'student', 'department', 'project', 'staff'])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "to take only sample of all files\n",
    "\"\"\"\n",
    "short_all_files = {}\n",
    "max_count = 2\n",
    "print(all_files.keys())\n",
    "for k, v in all_files.items():\n",
    "    if k not in short_all_files:\n",
    "            short_all_files[k] = []\n",
    "    short_all_files[k] = v[:max_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(short_all_files['student'][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_local = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not read_local:\n",
    "    raw = []\n",
    "    for k, v in all_files.items():\n",
    "        for fnames in v:\n",
    "            for fs in fnames:\n",
    "                with open(fs, 'rb') as f:\n",
    "                    raw_data = f.read()\n",
    "                    raw.append([raw_data, class_index[k]])\n",
    "\n",
    "    raw_df = pd.DataFrame(raw, columns=[\"text\", \"Class\"])\n",
    "else:\n",
    "    raw_df = pd.read_csv('raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8282, 2)\n"
     ]
    }
   ],
   "source": [
    "print(raw_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_df.to_csv('raw.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_features = 100\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=no_features)\n",
    "tfidf_vectorizer2 = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features)\n",
    "tfidf_vectorizer3 = TfidfVectorizer(ngram_range=(1, 2), max_df=0.95, min_df=2, max_features=no_features)\n",
    "tf_vectorizer = CountVectorizer(max_features=no_features)\n",
    "stopset = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(txt):\n",
    "    cleantext = BeautifulSoup(txt, \"lxml\").text\n",
    "    tokens = []\n",
    "    for token in wordpunct_tokenize(cleantext):\n",
    "        if token.isdigit():\n",
    "            continue\n",
    "        if all(char in string.punctuation for char in token):\n",
    "            continue\n",
    "        \n",
    "        token = token.lower()\n",
    "        token = token.strip()  # Strip whitespace and other punctuations\n",
    "        token = token.strip('_')  # remove _ if any\n",
    "        token = token.strip('*')\n",
    "        if token in stopset:\n",
    "            continue\n",
    "        tokens.append(token)\n",
    "        lemmatizer.lemmatize(token)\n",
    "#     x = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(txt, vectorizer):\n",
    "    X = vectorizer.fit_transform(txt)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_vectorization(txt):\n",
    "    X_tf = tf_vectorizer.fit_transform(txt)\n",
    "    return X_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = raw_df.sample(frac=0.1, replace=True)\n",
    "df = raw_df\n",
    "df['processed_text'] = df['text'].apply(process)\n",
    "df['processed_text_cnct'] = df['processed_text'].apply(lambda tokens: ' '.join(str(v) for v in tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[(df['Class'] ==  0) | (df['Class'] ==  1) | (df['Class'] ==  4) | (df['Class'] ==  5) | (df['Class'] ==  6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class\n",
      "0    1641\n",
      "1    1124\n",
      "2     137\n",
      "3     182\n",
      "4     930\n",
      "5     504\n",
      "6    3764\n",
      "dtype: int64\n",
      "(8282, 4)\n"
     ]
    }
   ],
   "source": [
    "class_counts = df.groupby(['Class']).size()\n",
    "print(class_counts)\n",
    "print(df.shape)\n",
    "# class_counts_raw = raw_df.groupby(['Class']).size()\n",
    "# print(class_counts_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 30000\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(df['processed_text_cnct'])\n",
    "sequences = tokenizer.texts_to_sequences(df['processed_text_cnct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 77428 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337.04503743057234 934.8175976412838\n"
     ]
    }
   ],
   "source": [
    "avg = sum( map(len, sequences) ) / len(sequences)\n",
    "std = np.sqrt(sum( map(lambda x: (len(x) - avg)**2, sequences)) / len(sequences))\n",
    "\n",
    "print(avg,std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average text is 300 in length, let's restrict sequence length to 150 words.\n",
    "max_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (8282, 100)\n",
      "Shape of labels: (8282, 7)\n"
     ]
    }
   ],
   "source": [
    "labels = to_categorical(np.asarray(df['Class']))\n",
    "print('Shape of data:', data.shape)\n",
    "print('Shape of labels:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14851,  4130,   610,   578,     7,  1142,   245,   683,   829,\n",
       "        4917,  4918,    28,  4917,  4918,  1350,   497,     6,    35,\n",
       "         195,   207,    28,     2,    72,   487,  1790,    72,   487,\n",
       "         497,   709,  4299,     2,    68,   709,   284,     3,    68,\n",
       "          84,   376,    28,     2,  2306,  3034,  2306,    80,   250,\n",
       "         709,  1944,     3,    68,    42,  1637,  2684,   306,    13,\n",
       "          33,     3,   245,     6,   733, 21662,     6,    13,    33,\n",
       "         198,     3,   668,   383,   846,     5, 19290,     3,    89,\n",
       "         151,   146,   262,     6,    13,    33,  1218,  1328,   480,\n",
       "       19289,    57,     6,  3109,    35,   195,     6,    57,     3,\n",
       "         575,   499,   846, 16019, 14852,  7609, 12373,   925,  1218,\n",
       "        1328], dtype=int32)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "glove_dir = 'data/glove'\n",
    "embeddings_index = {} # We create a dictionary of word -> embedding\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt')) # Open file\n",
    "# In the dataset, each line represents a new word embedding\n",
    "# The line starts with the word and the embedding values follow\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0] # The first value is the word, the rest are the values of the embedding\n",
    "    embedding = np.asarray(values[1:], dtype='float32') # Load embedding\n",
    "    embeddings_index[word] = embedding # Add embedding to our embedding dictionary\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.004451992, 0.4081574)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a matrix of all embeddings\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean = all_embs.mean() # Calculate mean\n",
    "emb_std = all_embs.std() # Calculate standard deviation\n",
    "emb_mean,emb_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100 # We use 250 dimensional glove vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words = min(vocab_size, len(word_index)) # How many words are there actually\n",
    "\n",
    "# Create a random matrix with the same mean and std as the embeddings\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embedding_dim))\n",
    "\n",
    "# The vectors need to be in the same position as their index. \n",
    "# Meaning a word with token 1 needs to be in the second row (rows start with zero) and so on\n",
    "\n",
    "# Loop over all words in the word index\n",
    "for word, i in word_index.items():\n",
    "    # If we are above the amount of words we want to use we do nothing\n",
    "    if i >= vocab_size: \n",
    "        continue\n",
    "    # Get the embedding vector for the word\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    # If there is an embedding vector, put it in the embedding matrix\n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data\n",
    "y = labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counts(y):\n",
    "    counts = {}\n",
    "    for row in y:\n",
    "        for idx, label_val in enumerate(row):\n",
    "            if idx not in counts:\n",
    "                counts[idx] = 0\n",
    "            if label_val == 1:\n",
    "                counts[idx] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1305, 1: 887, 2: 105, 3: 147, 4: 760, 5: 408, 6: 3013}\n",
      "{0: 336, 1: 237, 2: 32, 3: 35, 4: 170, 5: 96, 6: 751}\n"
     ]
    }
   ],
   "source": [
    "print(counts(y_train))\n",
    "print(counts(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_length, weights = [embedding_matrix], trainable = False))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(7))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 100)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 100, 100)          3000000   \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 100, 64)           42240     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 100, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 7)                 455       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 3,075,719\n",
      "Trainable params: 75,719\n",
      "Non-trainable params: 3,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[metrics.mae, metrics.categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5962 samples, validate on 663 samples\n",
      "Epoch 1/25\n",
      "5962/5962 [==============================] - 19s 3ms/step - loss: 1.8090 - mean_absolute_error: 0.2361 - categorical_accuracy: 0.3455 - val_loss: 1.5796 - val_mean_absolute_error: 0.2148 - val_categorical_accuracy: 0.4691\n",
      "Epoch 2/25\n",
      "5962/5962 [==============================] - 17s 3ms/step - loss: 1.5837 - mean_absolute_error: 0.2064 - categorical_accuracy: 0.4524 - val_loss: 1.4804 - val_mean_absolute_error: 0.2011 - val_categorical_accuracy: 0.4706\n",
      "Epoch 3/25\n",
      "5962/5962 [==============================] - 14s 2ms/step - loss: 1.5338 - mean_absolute_error: 0.2055 - categorical_accuracy: 0.4504 - val_loss: 1.4482 - val_mean_absolute_error: 0.2009 - val_categorical_accuracy: 0.4706\n",
      "Epoch 4/25\n",
      "5962/5962 [==============================] - 14s 2ms/step - loss: 1.4986 - mean_absolute_error: 0.2041 - categorical_accuracy: 0.4577 - val_loss: 1.3866 - val_mean_absolute_error: 0.1955 - val_categorical_accuracy: 0.4811\n",
      "Epoch 5/25\n",
      "5962/5962 [==============================] - 14s 2ms/step - loss: 1.4374 - mean_absolute_error: 0.1947 - categorical_accuracy: 0.4745 - val_loss: 1.3280 - val_mean_absolute_error: 0.1852 - val_categorical_accuracy: 0.5158\n",
      "Epoch 6/25\n",
      "5962/5962 [==============================] - 14s 2ms/step - loss: 1.3849 - mean_absolute_error: 0.1877 - categorical_accuracy: 0.4925 - val_loss: 1.2810 - val_mean_absolute_error: 0.1825 - val_categorical_accuracy: 0.5309\n",
      "Epoch 7/25\n",
      "5962/5962 [==============================] - 14s 2ms/step - loss: 1.3421 - mean_absolute_error: 0.1848 - categorical_accuracy: 0.5107 - val_loss: 1.2565 - val_mean_absolute_error: 0.1813 - val_categorical_accuracy: 0.5385\n",
      "Epoch 8/25\n",
      "5962/5962 [==============================] - 13s 2ms/step - loss: 1.2924 - mean_absolute_error: 0.1814 - categorical_accuracy: 0.5178 - val_loss: 1.1920 - val_mean_absolute_error: 0.1712 - val_categorical_accuracy: 0.5671\n",
      "Epoch 9/25\n",
      "5962/5962 [==============================] - 14s 2ms/step - loss: 1.2405 - mean_absolute_error: 0.1724 - categorical_accuracy: 0.5404 - val_loss: 1.1719 - val_mean_absolute_error: 0.1639 - val_categorical_accuracy: 0.5732\n",
      "Epoch 10/25\n",
      "5962/5962 [==============================] - 14s 2ms/step - loss: 1.2093 - mean_absolute_error: 0.1678 - categorical_accuracy: 0.5458 - val_loss: 1.1508 - val_mean_absolute_error: 0.1673 - val_categorical_accuracy: 0.5777\n",
      "Epoch 11/25\n",
      "5962/5962 [==============================] - 13s 2ms/step - loss: 1.1837 - mean_absolute_error: 0.1640 - categorical_accuracy: 0.5584 - val_loss: 1.0942 - val_mean_absolute_error: 0.1603 - val_categorical_accuracy: 0.6048\n",
      "Epoch 12/25\n",
      "5962/5962 [==============================] - 13s 2ms/step - loss: 1.1505 - mean_absolute_error: 0.1637 - categorical_accuracy: 0.5756 - val_loss: 1.0858 - val_mean_absolute_error: 0.1569 - val_categorical_accuracy: 0.6063\n",
      "Epoch 13/25\n",
      "5962/5962 [==============================] - 13s 2ms/step - loss: 1.1334 - mean_absolute_error: 0.1596 - categorical_accuracy: 0.5840 - val_loss: 1.0794 - val_mean_absolute_error: 0.1491 - val_categorical_accuracy: 0.5988\n",
      "Epoch 14/25\n",
      "5962/5962 [==============================] - 14s 2ms/step - loss: 1.1034 - mean_absolute_error: 0.1544 - categorical_accuracy: 0.5944 - val_loss: 1.0594 - val_mean_absolute_error: 0.1496 - val_categorical_accuracy: 0.6184\n",
      "Epoch 15/25\n",
      "5962/5962 [==============================] - 15s 3ms/step - loss: 1.0787 - mean_absolute_error: 0.1522 - categorical_accuracy: 0.6001 - val_loss: 1.0439 - val_mean_absolute_error: 0.1505 - val_categorical_accuracy: 0.6259\n",
      "Epoch 16/25\n",
      "5962/5962 [==============================] - 14s 2ms/step - loss: 1.0558 - mean_absolute_error: 0.1493 - categorical_accuracy: 0.6152 - val_loss: 1.0196 - val_mean_absolute_error: 0.1460 - val_categorical_accuracy: 0.6244\n",
      "Epoch 17/25\n",
      "5962/5962 [==============================] - 12s 2ms/step - loss: 1.0274 - mean_absolute_error: 0.1468 - categorical_accuracy: 0.6255 - val_loss: 0.9881 - val_mean_absolute_error: 0.1400 - val_categorical_accuracy: 0.6425\n",
      "Epoch 18/25\n",
      "5962/5962 [==============================] - 12s 2ms/step - loss: 1.0166 - mean_absolute_error: 0.1442 - categorical_accuracy: 0.6333 - val_loss: 0.9853 - val_mean_absolute_error: 0.1399 - val_categorical_accuracy: 0.6456\n",
      "Epoch 19/25\n",
      "5962/5962 [==============================] - 12s 2ms/step - loss: 0.9991 - mean_absolute_error: 0.1426 - categorical_accuracy: 0.6337 - val_loss: 0.9695 - val_mean_absolute_error: 0.1367 - val_categorical_accuracy: 0.6410\n",
      "Epoch 20/25\n",
      "5962/5962 [==============================] - 12s 2ms/step - loss: 0.9876 - mean_absolute_error: 0.1392 - categorical_accuracy: 0.6454 - val_loss: 0.9639 - val_mean_absolute_error: 0.1392 - val_categorical_accuracy: 0.6576\n",
      "Epoch 21/25\n",
      "5962/5962 [==============================] - 12s 2ms/step - loss: 0.9727 - mean_absolute_error: 0.1376 - categorical_accuracy: 0.6491 - val_loss: 0.9664 - val_mean_absolute_error: 0.1367 - val_categorical_accuracy: 0.6576\n",
      "Epoch 22/25\n",
      "5962/5962 [==============================] - 13s 2ms/step - loss: 0.9566 - mean_absolute_error: 0.1377 - categorical_accuracy: 0.6530 - val_loss: 0.9378 - val_mean_absolute_error: 0.1343 - val_categorical_accuracy: 0.6637\n",
      "Epoch 23/25\n",
      "5962/5962 [==============================] - 15s 2ms/step - loss: 0.9230 - mean_absolute_error: 0.1330 - categorical_accuracy: 0.6726 - val_loss: 0.9369 - val_mean_absolute_error: 0.1291 - val_categorical_accuracy: 0.6637\n",
      "Epoch 24/25\n",
      "5962/5962 [==============================] - 16s 3ms/step - loss: 0.9085 - mean_absolute_error: 0.1292 - categorical_accuracy: 0.6765 - val_loss: 0.9292 - val_mean_absolute_error: 0.1313 - val_categorical_accuracy: 0.6637\n",
      "Epoch 25/25\n",
      "5962/5962 [==============================] - 15s 3ms/step - loss: 0.8892 - mean_absolute_error: 0.1276 - categorical_accuracy: 0.6897 - val_loss: 0.9682 - val_mean_absolute_error: 0.1291 - val_categorical_accuracy: 0.6591\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x130032c50>"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=25, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_mod = []\n",
    "for row in y_pred:\n",
    "    max_val = max(row)\n",
    "    mod_label = []\n",
    "    for label_val in row:\n",
    "        if label_val < max_val:\n",
    "            mod_label.append(0.)\n",
    "        else:\n",
    "            mod_label.append(1.)\n",
    "    y_pred_mod.append(mod_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.06001716 0.67120445 0.01769655 0.02491975 0.02603431 0.04097567\n",
      "  0.1591521 ]]\n",
      "[[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "[[0. 1. 0. 0. 0. 0. 0.]]\n",
      "{0: 336, 1: 237, 2: 32, 3: 35, 4: 170, 5: 96, 6: 751}\n"
     ]
    }
   ],
   "source": [
    "print(y_pred[:1])\n",
    "print(y_pred_mod[:1])\n",
    "print(y_test[:1])\n",
    "print(counts(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[213  40   0   0   6   2  75]\n",
      " [ 39 125   0   5  13   4  51]\n",
      " [ 16   5   0   0   0   1  10]\n",
      " [  2   2   0  15   4   0  12]\n",
      " [  4   2   0   2 111   1  50]\n",
      " [  3   6   0   1   2  13  71]\n",
      " [ 27  29   0   3  75   5 612]]\n",
      "[ 91  84   0  11 100  13 269]\n",
      "accuracy [0.87085094 0.88171394 0.98068799 0.98129149 0.90404345 0.94206397\n",
      " 0.75377188]\n",
      "precision [0.70065789 0.59808612        nan 0.57692308 0.52606635 0.5\n",
      " 0.69466515]\n",
      "recall [0.63392857 0.52742616 0.         0.42857143 0.65294118 0.13541667\n",
      " 0.81491345]\n",
      "f1 [0.665625   0.56053812        nan 0.49180328 0.58267717 0.21311475\n",
      " 0.75      ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.70065789, 0.59808612,        nan, 0.57692308, 0.52606635,\n",
       "        0.5       , 0.69466515]),\n",
       " array([0.63392857, 0.52742616, 0.        , 0.42857143, 0.65294118,\n",
       "        0.13541667, 0.81491345]),\n",
       " array([0.87085094, 0.88171394, 0.98068799, 0.98129149, 0.90404345,\n",
       "        0.94206397, 0.75377188]),\n",
       " array([0.665625  , 0.56053812,        nan, 0.49180328, 0.58267717,\n",
       "        0.21311475, 0.75      ]))"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion_matrix(y_pred_mod, y_test)\n",
    "# cm = confusion_matrix(y_test.argmax(axis=1), np.array(y_pred_mod).argmax(axis=1))\n",
    "c_matrix(y_test, np.array(y_pred_mod), num_classes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8282, 5)\n"
     ]
    }
   ],
   "source": [
    "X_tf = vectorize(df['processed_text_cnct'], tfidf_vectorizer3)\n",
    "\n",
    "no_topics = 5\n",
    "num_iter = 5\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=no_topics, max_iter=num_iter, learning_method='online', learning_offset=50.,random_state=9, evaluate_every=100).fit(X_tf)\n",
    "\n",
    "lda_x = lda.transform(X_tf)\n",
    "print(lda_x.shape)\n",
    "\n",
    "tf_feature_names = tfidf_vectorizer3.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "computer science university computer science page home home page research department ni\n",
      "Topic 1:\n",
      "nthe program one class use file course code problem time\n",
      "Topic 2:\n",
      "ndate version length nlast html ncontent jan gmt ncontent nlast modified length ncontent length\n",
      "Topic 3:\n",
      "nserver ncsa ncsa tue date ncsa ncontent gmt nserver html nlast page nov date tue\n",
      "Topic 4:\n",
      "systems research parallel system computer software project data design distributed\n"
     ]
    }
   ],
   "source": [
    "no_top_words = 10\n",
    "topic_indices = []\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        top_features = [feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "        print(\" \".join(top_features))\n",
    "\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf = vectorize(df['processed_text_cnct'], tfidf_vectorizer3)\n",
    "X = X_tfidf\n",
    "# X = X.todense()\n",
    "y = df['Class']\n",
    "# X = lda_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7963, 100)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler(with_mean = False).fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 in train set: 1231\n",
      "#0 in test set: 410\n",
      "#1 in train set: 843\n",
      "#1 in test set: 281\n",
      "#2 in train set: 0\n",
      "#2 in test set: 0\n",
      "#3 in train set: 0\n",
      "#3 in test set: 0\n",
      "#4 in train set: 697\n",
      "#4 in test set: 233\n",
      "#5 in train set: 378\n",
      "#5 in test set: 126\n",
      "#6 in train set: 2823\n",
      "#6 in test set: 941\n"
     ]
    }
   ],
   "source": [
    "for i in range(7):\n",
    "    print(\"#{} in train set: {}\".format(i, len(y_train[y_train == i])))\n",
    "    print(\"#{} in test set: {}\".format(i, len(y_test[y_test == i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.64 (+/- 0.02) [Naive Bayes]\n",
      "Accuracy: 0.74 (+/- 0.04) [SVM Linear]\n",
      "Accuracy: 0.75 (+/- 0.04) [SVM RBF]\n",
      "Accuracy: 0.74 (+/- 0.04) [Ensemble]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'grid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-153-7a0d07d3b9c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f1_micro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: %0.2f (+/- %0.2f) [%s]\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0meclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'grid' is not defined"
     ]
    }
   ],
   "source": [
    "scoring = ['precision_macro', 'recall_macro']\n",
    "estimators = []\n",
    "clf1 = MultinomialNB()\n",
    "estimators.append(clf1)\n",
    "clf2 = svm.SVC(C=100, kernel='linear')\n",
    "estimators.append(clf2)\n",
    "clf3 = svm.SVC(C=100, kernel='rbf', gamma=0.01)\n",
    "estimators.append(clf3)\n",
    "eclf = VotingClassifier(estimators=[('nb', clf1), ('svml', clf2), ('svmr', clf3)], voting='hard')\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['Naive Bayes', 'SVM Linear', 'SVM RBF', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, X, y, cv=5, scoring='f1_micro')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('nb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)), ('svml', SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))],\n",
       "         flatten_transform=None, n_jobs=None, voting='hard', weights=None)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clf = svm.SVC(C=100, kernel='linear')\n",
    "# clf = MultinomialNB()\n",
    "clf = eclf\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[573 478 366 271 837]\n"
     ]
    }
   ],
   "source": [
    "# print(clf.n_support_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_matrix(y_true, y_pred, num_classes=7):\n",
    "    cm = confusion_matrix(y_true.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "    print(cm)\n",
    "    cm_np = np.asarray(cm)\n",
    "    TP = np.diag(cm_np)\n",
    "#     print(TP)\n",
    "    FP = np.sum(cm, axis=0) - TP\n",
    "    print(FP)\n",
    "    FN = np.sum(cm, axis=1) - TP\n",
    "#     print(FN)\n",
    "    TN = []\n",
    "    for i in range(num_classes):\n",
    "        temp = np.delete(cm, i, 0)    # delete ith row\n",
    "        temp = np.delete(temp, i, 1)  # delete ith column\n",
    "        TN.append(sum(sum(temp)))\n",
    "#     print(TN)\n",
    "    prec = TP/(TP+FP)\n",
    "    rec = TP/(TP+FN)\n",
    "    acc = (TP+TN)/(TP+FP+TN+FN)\n",
    "    f1 = 2*prec*rec/(prec+rec)\n",
    "\n",
    "    print(\"accuracy\", acc)\n",
    "    print(\"precision\", prec)\n",
    "    print(\"recall\", rec)\n",
    "    print(\"f1\", f1)\n",
    "    return prec, rec, acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2  15   0   0  73   0 246]\n",
      " [  1   5   0   1  62   0 168]\n",
      " [  0   1   0   0  10   0  21]\n",
      " [  0   1   0   0  11   0  23]\n",
      " [  1   8   0   0  45   0 116]\n",
      " [  1   4   0   0  29   0  62]\n",
      " [  7  23   1   0 209   0 511]]\n"
     ]
    }
   ],
   "source": [
    "# cm = confusion_matrix(y_true, y_pred)\n",
    "cm = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "print(cm)\n",
    "cm_np = np.asarray(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 670  439    4   59  354  108 1681]\n"
     ]
    }
   ],
   "source": [
    "TP = np.diag(cm_np)\n",
    "print(TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-658 -382   -3  -58   85 -108 -534]\n"
     ]
    }
   ],
   "source": [
    "FP = np.sum(cm, axis=0) - TP\n",
    "print(FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 60  40  14  10  49  33 120]\n"
     ]
    }
   ],
   "source": [
    "FN = np.sum(cm, axis=1) - TP\n",
    "print(FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[592, 644, 813, 804, 699, 738, 358]\n"
     ]
    }
   ],
   "source": [
    "num_classes = 7\n",
    "TN = []\n",
    "for i in range(num_classes):\n",
    "    temp = np.delete(cm, i, 0)    # delete ith row\n",
    "    temp = np.delete(temp, i, 1)  # delete ith column\n",
    "    TN.append(sum(sum(temp)))\n",
    "print(TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy [0.83956574 0.86489747 0.98069964 0.97949337 0.89626055 0.91073583\n",
      " 0.74185766]\n",
      "precision [0.58757062 0.50344828 0.         0.53333333 0.54320988 0.29310345\n",
      " 0.73219373]\n",
      "recall [0.63414634 0.6460177  0.         0.44444444 0.47311828 0.34\n",
      " 0.68169761]\n",
      "f1 [0.60997067 0.56589147        nan 0.48484848 0.50574713 0.31481481\n",
      " 0.70604396]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in true_divide\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "prec = TP/(TP+FP)\n",
    "rec = TP/(TP+FN)\n",
    "acc = (TP+TN)/(TP+FP+TN+FN)\n",
    "f1 = 2*prec*rec/(prec+rec)\n",
    "\n",
    "print(\"accuracy\", acc)\n",
    "print(\"precision\", prec)\n",
    "print(\"recall\", rec)\n",
    "print(\"f1\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_eval_metrics(x, y, y2, y3, y4, img_name):\n",
    "    fig, ax = plt.subplots()\n",
    "    y_label = \"value\"\n",
    "    x_label = \"classes\"\n",
    "    plt.plot(x, y, color='b', marker='o', label=\"Accuracy\")\n",
    "    plt.plot(x, y2, color='g', marker='+', label=\"Precision\")\n",
    "    plt.plot(x, y3, color='y', marker='x', label=\"Recall\")\n",
    "    plt.plot(x, y4, color='r', marker='s', label=\"F1-measure\")\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.legend()\n",
    "#     plt.scatter(x, y, label=\"Accuracy\")\n",
    "#     plt.scatter(x, y2, label=\"Precision\")\n",
    "#     plt.scatter(x, y3, label=\"Recall\")\n",
    "#     plt.scatter(x, y4, label=\"F1-measure\")\n",
    "    plt.show()\n",
    "    fig.savefig(img_name)\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_eval_metrics(classes, acc, prec, rec, f1, \"tfidf-mNB.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
