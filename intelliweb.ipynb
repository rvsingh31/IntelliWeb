{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, glob, string, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation, Embedding, Dropout, TimeDistributed\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras import metrics\n",
    "import tensorflow as tf\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from numpy.random import seed\n",
    "seed(9)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"student\", \"faculty\", \"staff\", \"department\", \"course\", \"project\", \"other\"]\n",
    "class_index = dict((c, i) for i, c in enumerate(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'student': 0, 'faculty': 1, 'staff': 2, 'department': 3, 'course': 4, 'project': 5, 'other': 6}\n"
     ]
    }
   ],
   "source": [
    "print(class_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create list of all files, along with their classes\n",
    "all_files varia\n",
    "all_files : {\"student\": [[\"file_path\"],[\"file_path\"]], \"course\": [[\"file_path2\", \"file_path3\"]]}\n",
    "\"\"\"\n",
    "all_files = {}\n",
    "path = \"data/raw/webkb/\"\n",
    "all_folders = os.listdir(path)\n",
    "for clz in all_folders:\n",
    "    if clz.startswith('.'):\n",
    "        continue\n",
    "    if clz not in all_files:\n",
    "        all_files[clz] = []\n",
    "    path_with_clz = path + clz + '/'\n",
    "    all_univs = os.listdir(path_with_clz)\n",
    "    for univ in all_univs:\n",
    "        if univ.startswith('.'):\n",
    "            continue\n",
    "        path_with_univs = path_with_clz + univ + '/'\n",
    "        all_files[clz].append(glob.glob(os.path.join(path_with_univs, '*')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(all_files[\"department\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['faculty', 'course', 'other', 'student', 'department', 'project', 'staff'])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "to take only sample of all files\n",
    "\"\"\"\n",
    "short_all_files = {}\n",
    "max_count = 2\n",
    "print(all_files.keys())\n",
    "for k, v in all_files.items():\n",
    "    if k not in short_all_files:\n",
    "            short_all_files[k] = []\n",
    "    short_all_files[k] = v[:max_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(short_all_files['student'][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_local = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not read_local:\n",
    "    raw = []\n",
    "    for k, v in all_files.items():\n",
    "        for fnames in v:\n",
    "            for fs in fnames:\n",
    "                with open(fs, 'rb') as f:\n",
    "                    raw_data = f.read()\n",
    "                    raw.append([raw_data, class_index[k]])\n",
    "\n",
    "    raw_df = pd.DataFrame(raw, columns=[\"text\", \"Class\"])\n",
    "else:\n",
    "    raw_df = pd.read_csv('raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8282, 2)\n"
     ]
    }
   ],
   "source": [
    "print(raw_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_df.to_csv('raw.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_features = 100\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=no_features)\n",
    "tfidf_vectorizer2 = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features)\n",
    "tfidf_vectorizer3 = TfidfVectorizer(ngram_range=(1, 2), max_df=0.95, min_df=2, max_features=no_features)\n",
    "tf_vectorizer = CountVectorizer(max_features=no_features)\n",
    "stopset = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(txt):\n",
    "    cleantext = BeautifulSoup(txt, \"lxml\").text\n",
    "    tokens = []\n",
    "    for token in wordpunct_tokenize(cleantext):\n",
    "        if token.isdigit():\n",
    "            continue\n",
    "        if all(char in string.punctuation for char in token):\n",
    "            continue\n",
    "        \n",
    "        token = token.lower()\n",
    "        token = token.strip()  # Strip whitespace and other punctuations\n",
    "        token = token.strip('_')  # remove _ if any\n",
    "        token = token.strip('*')\n",
    "        if token in stopset:\n",
    "            continue\n",
    "        tokens.append(token)\n",
    "        lemmatizer.lemmatize(token)\n",
    "#     x = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(txt, vectorizer):\n",
    "    X = vectorizer.fit_transform(txt)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_vectorization(txt):\n",
    "    X_tf = tf_vectorizer.fit_transform(txt)\n",
    "    return X_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = raw_df[(raw_df['Class'] != 6) & (raw_df['Class'] != 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4381, 2)\n",
      "[0, 1, 4, 5, 3]\n"
     ]
    }
   ],
   "source": [
    "print(new_df.shape)\n",
    "print(new_df['Class'].value_counts().index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# df = raw_df.sample(frac=0.1, replace=True)\n",
    "df = new_df\n",
    "# df = raw_df\n",
    "df['processed_text'] = df['text'].apply(process)\n",
    "df['processed_text_cnct'] = df['processed_text'].apply(lambda tokens: ' '.join(str(v) for v in tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mime version server cern date wednesday nov gmt content type text html content length last modified wednesday jun gmt bruce randall donald bruce randall donald associate professor brd cs cornell edu ph mit weather palo alto official departmental home page cornell robotics vision laboratory dan huttenlocher founded cornell robotics vision laboratory research interests include robotics microelectromechanical systems geometric algorithms artificial intelligence robotics science seeks forge intelligent computational connection perception action working graduate student jim jennings research associate daniela rus graduate student russell brown lab alumnus jonathan rees mit developed team autonomous mobile robots perform sophisticated distributed manipulation tasks moving furniture robots run robust spmd protocols completely asynchronous require communication grad student karl böhringer ee professor noel macdonald building massively parallel array microactuators cornell national nanofabrication laboratory array scream chip containing actuators square centemeter orient small parts without sensory feedback microfabricated actuator arrays could used construct programmable parts feeders scale build self propelled ic walking vlsi chips graduate student amy briggs worked dan huttenlocher vision group develop sensor planning surveillance system team mobile robots robots use board vision detect intercept targets lab demos massively parallel micro fabricated actuator arrays mpeg video tommy chasing lily tommy lily mobile robots built using algorithms developed vision group lab lily track tommy follow using visual information alone video shows lily view chase face morphing selected recent publications k f böhringer b r donald n c macdonald upper lower bounds programmable vector fields applications mems vibratory parts feeders international workshop algorithmic foundations robotics toulouse france j briggs b r donald robust geometric algorithms sensor planning international workshop algorithmic foundations robotics toulouse france k f böhringer b r donald n c macdonald single crystal silicon actuator arrays micro manipulation tasks ieee workshop micro electro mechanical systems mems san diego california february k f böhringer b r donald n c macdonald classification lower bounds mems arrays vibratory parts feeders programmable vector fields cannot part ieee international conference robotics automation icra minneapolis minnesota april k f böhringer b r donald n c macdonald new improved manipulation algorithms mems arrays vibratory parts feeders programmable vector fields cannot part ii ieee international conference robotics automation icra minneapolis minnesota april provably good approximation algorithms optimal kinodynamic planning robots decoupled dynamics bounds p xavier algorithmica vol pp provably good approximation algorithms optimal kinodynamic planning cartesian robots open chain manipulators p xavier algorithmica vol pp kinodynamic motion planning p xavier j canny j reif journal acm vol nov pp information invariants distributed manipulation j jennings rus international journal robotics research press b r donald j jennings rus minimalism distribution supermodularity journal experimental theoretical artificial intelligence jetai press writing book entitled information invariants robotics draft first quarter book appeared paper artificial intelligence information invariants robotics revised ms based paper information invariants robotics artificial intelligence vol jan pp distributed robotic manipulation experiments minimalism international symposium experimental robotics iser stanford ca moving furniture teams automonous mobile robots j jennings rus proc ieee robotics society japan international workshop intelligent robots systems iros pittsburgh pa sensorless manipulation using massively parallel micro fabricated actuator arrays k f böhringer r mihailovich noel c macdonald proc ieee international conference robotics automation san diego ca may demo detailed explanation program mobile robots scheme j rees proc ieee international conference robotics automation nice france may pp information invariants distributed manipulation j jennings rus first workshop algorithmic foundations robotics k peters boston ed r wilson j c latombe automatic sensor configuration task directed planning amy briggs proceedings ieee international conference robotics automation san diego ca may publications recent theses papers phd students patrick xavier phd except thesis tr mostly superseded three recent journal papers listed starting amy briggs phd papers thesis russell brown phd papers thesis jim jennings karl f böhringer post docs trained lab daniela rus jonathan rees dinesh pai papers avalable cornell cs tr server papers listed cornell cs tr version online tech reports cornell library catalog cs tr index obtaining copies papers copies papers available via anonymous ftp pictures developed team small autonomous mobile robots move furniture around lab group portrait robots click see picture tommy lily mobot pushing couch click see picture tommy lily rotating couch click see picture tommy mobile robot drawn loretta pompilio click see pictures people robots working lab lab discovery channel beyond find fun poem alfred mail agent family pictures play harmful swallowed ithaca sometimes play california people robots cornell robotics vision laboratory cornell robotics vision laboratory home page people cornell cs department click search tools information access stuff return cornell cs top level say click tallest darkest leading man hollywood merian c cooper fay wray'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['processed_text_cnct'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[(df['Class'] ==  0) | (df['Class'] ==  1) | (df['Class'] ==  4) | (df['Class'] ==  5) | (df['Class'] ==  6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class\n",
      "0    1641\n",
      "1    1124\n",
      "2     137\n",
      "3     182\n",
      "4     930\n",
      "5     504\n",
      "dtype: int64\n",
      "(4518, 4)\n"
     ]
    }
   ],
   "source": [
    "class_counts = df.groupby(['Class']).size()\n",
    "print(class_counts)\n",
    "print(df.shape)\n",
    "# class_counts_raw = raw_df.groupby(['Class']).size()\n",
    "# print(class_counts_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(df['processed_text_cnct'])\n",
    "sequences = tokenizer.texts_to_sequences(df['processed_text_cnct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 45067 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.96812749003984 42.959613847746034\n"
     ]
    }
   ],
   "source": [
    "avg = sum( map(len, sequences) ) / len(sequences)\n",
    "std = np.sqrt(sum( map(lambda x: (len(x) - avg)**2, sequences)) / len(sequences))\n",
    "\n",
    "print(avg,std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average text is 300 in length, let's restrict sequence length to 150 words.\n",
    "max_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (4518, 100)\n",
      "Shape of labels: (4518, 6)\n"
     ]
    }
   ],
   "source": [
    "labels = to_categorical(np.asarray(df['Class']))\n",
    "print('Shape of data:', data.shape)\n",
    "print('Shape of labels:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([46,  5, 11, 78,  6,  8,  1, 18, 23, 18, 44, 18, 44, 30,  2, 30, 23,\n",
       "       58, 98, 39, 38, 30, 17, 47, 49, 39, 30, 58, 61, 47, 69, 32,  1,  4,\n",
       "       52, 30, 35, 18, 93, 23, 58, 44, 54, 23, 17, 23, 61,  4, 30, 31, 27,\n",
       "       61,  6, 44, 44,  1, 27,  1,  4,  6,  1,  4,  1,  4, 31, 27, 18,  1,\n",
       "        4, 27, 30,  1,  4,  6, 27, 18,  6, 17,  6, 38, 38, 27, 27, 32,  6,\n",
       "       50, 82, 27, 18,  1,  4,  1,  4, 51, 81,  1,  4, 23, 17, 23],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "glove_dir = 'data/glove'\n",
    "embeddings_index = {} # We create a dictionary of word -> embedding\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt')) # Open file\n",
    "# In the dataset, each line represents a new word embedding\n",
    "# The line starts with the word and the embedding values follow\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0] # The first value is the word, the rest are the values of the embedding\n",
    "    embedding = np.asarray(values[1:], dtype='float32') # Load embedding\n",
    "    embeddings_index[word] = embedding # Add embedding to our embedding dictionary\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.004451992, 0.4081574)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a matrix of all embeddings\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean = all_embs.mean() # Calculate mean\n",
    "emb_std = all_embs.std() # Calculate standard deviation\n",
    "emb_mean,emb_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100 # We use 250 dimensional glove vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words = min(vocab_size, len(word_index)) # How many words are there actually\n",
    "\n",
    "# Create a random matrix with the same mean and std as the embeddings\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embedding_dim))\n",
    "\n",
    "# The vectors need to be in the same position as their index. \n",
    "# Meaning a word with token 1 needs to be in the second row (rows start with zero) and so on\n",
    "\n",
    "# Loop over all words in the word index\n",
    "for word, i in word_index.items():\n",
    "    # If we are above the amount of words we want to use we do nothing\n",
    "    if i >= vocab_size: \n",
    "        continue\n",
    "    # Get the embedding vector for the word\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    # If there is an embedding vector, put it in the embedding matrix\n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_matrix(y_true, y_pred, num_classes=7):\n",
    "    cm = confusion_matrix(y_true.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "    print(cm)\n",
    "    cm_np = np.asarray(cm)\n",
    "    TP = np.diag(cm_np)\n",
    "#     print(TP)\n",
    "    FP = np.sum(cm, axis=0) - TP\n",
    "#     print(FP)\n",
    "    FN = np.sum(cm, axis=1) - TP\n",
    "#     print(FN)\n",
    "    TN = []\n",
    "    for i in range(num_classes):\n",
    "        temp = np.delete(cm, i, 0)    # delete ith row\n",
    "        temp = np.delete(temp, i, 1)  # delete ith column\n",
    "        TN.append(sum(sum(temp)))\n",
    "#     print(TN)\n",
    "    prec = TP/(TP+FP)\n",
    "    rec = TP/(TP+FN)\n",
    "    acc = (TP+TN)/(TP+FP+TN+FN)\n",
    "    f1 = 2*prec*rec/(prec+rec)\n",
    "\n",
    "    print(\"accuracy\", acc)\n",
    "    print(\"precision\", prec)\n",
    "    print(\"recall\", rec)\n",
    "    print(\"f1\", f1)\n",
    "    return prec, rec, acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data\n",
    "y = labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counts(y):\n",
    "    counts = {}\n",
    "    for row in y:\n",
    "        for idx, label_val in enumerate(row):\n",
    "            if idx not in counts:\n",
    "                counts[idx] = 0\n",
    "            if label_val == 1:\n",
    "                counts[idx] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1322, 1: 884, 2: 108, 3: 149, 4: 741, 5: 410}\n",
      "{0: 319, 1: 240, 2: 29, 3: 33, 4: 189, 5: 94}\n",
      "(4518, 6)\n"
     ]
    }
   ],
   "source": [
    "print(counts(y_train))\n",
    "print(counts(y_test))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_length, weights = [embedding_matrix], trainable = False))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1]))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 100)          10000     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100, 128)          117248    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 774       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 259,606\n",
      "Trainable params: 249,606\n",
      "Non-trainable params: 10,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[metrics.mae, metrics.categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3252 samples, validate on 362 samples\n",
      "Epoch 1/25\n",
      "3252/3252 [==============================] - 14s 4ms/step - loss: 0.4965 - mean_absolute_error: 0.0757 - categorical_accuracy: 0.8346 - val_loss: 0.5471 - val_mean_absolute_error: 0.0893 - val_categorical_accuracy: 0.7983\n",
      "Epoch 2/25\n",
      "3252/3252 [==============================] - 14s 4ms/step - loss: 0.4671 - mean_absolute_error: 0.0824 - categorical_accuracy: 0.8423 - val_loss: 0.5214 - val_mean_absolute_error: 0.0779 - val_categorical_accuracy: 0.8315\n",
      "Epoch 3/25\n",
      "3252/3252 [==============================] - 19s 6ms/step - loss: 0.4348 - mean_absolute_error: 0.0705 - categorical_accuracy: 0.8518 - val_loss: 0.5143 - val_mean_absolute_error: 0.0754 - val_categorical_accuracy: 0.8315\n",
      "Epoch 4/25\n",
      "3252/3252 [==============================] - 18s 6ms/step - loss: 0.4121 - mean_absolute_error: 0.0700 - categorical_accuracy: 0.8595 - val_loss: 0.5743 - val_mean_absolute_error: 0.0838 - val_categorical_accuracy: 0.8066\n",
      "Epoch 5/25\n",
      "3252/3252 [==============================] - 18s 5ms/step - loss: 0.4139 - mean_absolute_error: 0.0673 - categorical_accuracy: 0.8616 - val_loss: 0.5423 - val_mean_absolute_error: 0.0751 - val_categorical_accuracy: 0.8149\n",
      "Epoch 6/25\n",
      "3252/3252 [==============================] - 16s 5ms/step - loss: 0.4215 - mean_absolute_error: 0.0699 - categorical_accuracy: 0.8576 - val_loss: 0.5189 - val_mean_absolute_error: 0.0775 - val_categorical_accuracy: 0.8232\n",
      "Epoch 7/25\n",
      "3252/3252 [==============================] - 17s 5ms/step - loss: 0.4341 - mean_absolute_error: 0.0701 - categorical_accuracy: 0.8549 - val_loss: 0.5279 - val_mean_absolute_error: 0.0791 - val_categorical_accuracy: 0.8149\n",
      "Epoch 8/25\n",
      "3252/3252 [==============================] - 17s 5ms/step - loss: 0.3848 - mean_absolute_error: 0.0675 - categorical_accuracy: 0.8718 - val_loss: 0.5393 - val_mean_absolute_error: 0.0770 - val_categorical_accuracy: 0.8232\n",
      "Epoch 9/25\n",
      "3252/3252 [==============================] - 15s 5ms/step - loss: 0.3975 - mean_absolute_error: 0.0664 - categorical_accuracy: 0.8678 - val_loss: 0.6159 - val_mean_absolute_error: 0.0733 - val_categorical_accuracy: 0.8122\n",
      "Epoch 10/25\n",
      "3252/3252 [==============================] - 16s 5ms/step - loss: 0.4202 - mean_absolute_error: 0.0645 - categorical_accuracy: 0.8629 - val_loss: 0.5617 - val_mean_absolute_error: 0.0797 - val_categorical_accuracy: 0.8094\n",
      "Epoch 11/25\n",
      "3252/3252 [==============================] - 16s 5ms/step - loss: 0.4303 - mean_absolute_error: 0.0703 - categorical_accuracy: 0.8564 - val_loss: 0.5601 - val_mean_absolute_error: 0.0848 - val_categorical_accuracy: 0.8260\n",
      "Epoch 12/25\n",
      "3252/3252 [==============================] - 14s 4ms/step - loss: 0.3996 - mean_absolute_error: 0.0690 - categorical_accuracy: 0.8635 - val_loss: 0.5611 - val_mean_absolute_error: 0.0760 - val_categorical_accuracy: 0.8122\n",
      "Epoch 13/25\n",
      "3252/3252 [==============================] - 17s 5ms/step - loss: 0.4090 - mean_absolute_error: 0.0688 - categorical_accuracy: 0.8635 - val_loss: 0.4953 - val_mean_absolute_error: 0.0771 - val_categorical_accuracy: 0.8232\n",
      "Epoch 14/25\n",
      "3252/3252 [==============================] - 15s 5ms/step - loss: 0.3816 - mean_absolute_error: 0.0648 - categorical_accuracy: 0.8690 - val_loss: 0.4957 - val_mean_absolute_error: 0.0715 - val_categorical_accuracy: 0.8315\n",
      "Epoch 15/25\n",
      "3252/3252 [==============================] - 14s 4ms/step - loss: 0.3401 - mean_absolute_error: 0.0594 - categorical_accuracy: 0.8875 - val_loss: 0.5260 - val_mean_absolute_error: 0.0702 - val_categorical_accuracy: 0.8232\n",
      "Epoch 16/25\n",
      "3252/3252 [==============================] - 14s 4ms/step - loss: 0.3581 - mean_absolute_error: 0.0584 - categorical_accuracy: 0.8770 - val_loss: 0.4705 - val_mean_absolute_error: 0.0690 - val_categorical_accuracy: 0.8398\n",
      "Epoch 17/25\n",
      "3252/3252 [==============================] - 14s 4ms/step - loss: 0.3284 - mean_absolute_error: 0.0569 - categorical_accuracy: 0.8831 - val_loss: 0.4755 - val_mean_absolute_error: 0.0718 - val_categorical_accuracy: 0.8481\n",
      "Epoch 18/25\n",
      "3252/3252 [==============================] - 14s 4ms/step - loss: 0.3242 - mean_absolute_error: 0.0574 - categorical_accuracy: 0.8893 - val_loss: 0.4693 - val_mean_absolute_error: 0.0683 - val_categorical_accuracy: 0.8536\n",
      "Epoch 19/25\n",
      "3252/3252 [==============================] - 14s 4ms/step - loss: 0.3140 - mean_absolute_error: 0.0550 - categorical_accuracy: 0.8902 - val_loss: 0.4864 - val_mean_absolute_error: 0.0706 - val_categorical_accuracy: 0.8564\n",
      "Epoch 20/25\n",
      "3252/3252 [==============================] - 15s 4ms/step - loss: 0.3011 - mean_absolute_error: 0.0533 - categorical_accuracy: 0.9001 - val_loss: 0.4556 - val_mean_absolute_error: 0.0635 - val_categorical_accuracy: 0.8536\n",
      "Epoch 21/25\n",
      "3252/3252 [==============================] - 15s 5ms/step - loss: 0.2882 - mean_absolute_error: 0.0498 - categorical_accuracy: 0.9025 - val_loss: 0.4898 - val_mean_absolute_error: 0.0642 - val_categorical_accuracy: 0.8370\n",
      "Epoch 22/25\n",
      "3252/3252 [==============================] - 14s 4ms/step - loss: 0.3162 - mean_absolute_error: 0.0538 - categorical_accuracy: 0.8994 - val_loss: 0.5164 - val_mean_absolute_error: 0.0749 - val_categorical_accuracy: 0.8370\n",
      "Epoch 23/25\n",
      "3252/3252 [==============================] - 14s 4ms/step - loss: 0.2976 - mean_absolute_error: 0.0523 - categorical_accuracy: 0.9001 - val_loss: 0.4645 - val_mean_absolute_error: 0.0624 - val_categorical_accuracy: 0.8398\n",
      "Epoch 24/25\n",
      "3252/3252 [==============================] - 14s 4ms/step - loss: 0.2931 - mean_absolute_error: 0.0481 - categorical_accuracy: 0.8964 - val_loss: 0.4621 - val_mean_absolute_error: 0.0671 - val_categorical_accuracy: 0.8619\n",
      "Epoch 25/25\n",
      "3252/3252 [==============================] - 14s 4ms/step - loss: 0.2833 - mean_absolute_error: 0.0505 - categorical_accuracy: 0.9068 - val_loss: 0.5015 - val_mean_absolute_error: 0.0672 - val_categorical_accuracy: 0.8453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13bf9f320>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=25, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_mod = []\n",
    "for row in y_pred:\n",
    "    max_val = max(row)\n",
    "    mod_label = []\n",
    "    for label_val in row:\n",
    "        if label_val < max_val:\n",
    "            mod_label.append(0.)\n",
    "        else:\n",
    "            mod_label.append(1.)\n",
    "    y_pred_mod.append(mod_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1683063  0.0352697  0.00945248 0.00330137 0.7212693  0.0624008 ]]\n",
      "[[0.0, 0.0, 0.0, 0.0, 1.0, 0.0]]\n",
      "[[0. 0. 0. 0. 1. 0.]]\n",
      "{0: 319, 1: 240, 2: 29, 3: 33, 4: 189, 5: 94}\n"
     ]
    }
   ],
   "source": [
    "print(y_pred[:1])\n",
    "print(y_pred_mod[:1])\n",
    "print(y_test[:1])\n",
    "print(counts(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[281  24   0   6   2   6]\n",
      " [ 16 212   1   5   4   2]\n",
      " [ 14   8   2   0   2   3]\n",
      " [  2   2   0  28   1   0]\n",
      " [  4   6   0   2 172   5]\n",
      " [  6   8   1   3   1  75]]\n",
      "accuracy [0.91150442 0.9159292  0.96792035 0.97676991 0.97013274 0.96128319]\n",
      "precision [0.86996904 0.81538462 0.5        0.63636364 0.94505495 0.82417582]\n",
      "recall [0.88087774 0.88333333 0.06896552 0.84848485 0.91005291 0.79787234]\n",
      "f1 [0.87538941 0.848      0.12121212 0.72727273 0.92722372 0.81081081]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.86996904, 0.81538462, 0.5       , 0.63636364, 0.94505495,\n",
       "        0.82417582]),\n",
       " array([0.88087774, 0.88333333, 0.06896552, 0.84848485, 0.91005291,\n",
       "        0.79787234]),\n",
       " array([0.91150442, 0.9159292 , 0.96792035, 0.97676991, 0.97013274,\n",
       "        0.96128319]),\n",
       " array([0.87538941, 0.848     , 0.12121212, 0.72727273, 0.92722372,\n",
       "        0.81081081]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion_matrix(y_pred_mod, y_test)\n",
    "# cm = confusion_matrix(y_test.argmax(axis=1), np.array(y_pred_mod).argmax(axis=1))\n",
    "c_matrix(y_test, np.array(y_pred_mod), num_classes=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8282, 5)\n"
     ]
    }
   ],
   "source": [
    "X_tf = vectorize(df['processed_text_cnct'], tfidf_vectorizer3)\n",
    "\n",
    "no_topics = 5\n",
    "num_iter = 5\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=no_topics, max_iter=num_iter, learning_method='online', learning_offset=50.,random_state=9, evaluate_every=100).fit(X_tf)\n",
    "\n",
    "lda_x = lda.transform(X_tf)\n",
    "print(lda_x.shape)\n",
    "\n",
    "tf_feature_names = tfidf_vectorizer3.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "computer science university computer science page home home page research department ni\n",
      "Topic 1:\n",
      "nthe program one class use file course code problem time\n",
      "Topic 2:\n",
      "ndate version length nlast html ncontent jan gmt ncontent nlast modified length ncontent length\n",
      "Topic 3:\n",
      "nserver ncsa ncsa tue date ncsa ncontent gmt nserver html nlast page nov date tue\n",
      "Topic 4:\n",
      "systems research parallel system computer software project data design distributed\n"
     ]
    }
   ],
   "source": [
    "no_top_words = 10\n",
    "topic_indices = []\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        top_features = [feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "        print(\" \".join(top_features))\n",
    "\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf = vectorize(df['processed_text_cnct'], tfidf_vectorizer3)\n",
    "X = X_tfidf\n",
    "# X = X.todense()\n",
    "y = df['Class']\n",
    "# X = lda_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7963, 100)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler(with_mean = False).fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 in train set: 1231\n",
      "#0 in test set: 410\n",
      "#1 in train set: 843\n",
      "#1 in test set: 281\n",
      "#2 in train set: 0\n",
      "#2 in test set: 0\n",
      "#3 in train set: 0\n",
      "#3 in test set: 0\n",
      "#4 in train set: 697\n",
      "#4 in test set: 233\n",
      "#5 in train set: 378\n",
      "#5 in test set: 126\n",
      "#6 in train set: 2823\n",
      "#6 in test set: 941\n"
     ]
    }
   ],
   "source": [
    "for i in range(7):\n",
    "    print(\"#{} in train set: {}\".format(i, len(y_train[y_train == i])))\n",
    "    print(\"#{} in test set: {}\".format(i, len(y_test[y_test == i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.64 (+/- 0.02) [Naive Bayes]\n",
      "Accuracy: 0.74 (+/- 0.04) [SVM Linear]\n",
      "Accuracy: 0.75 (+/- 0.04) [SVM RBF]\n",
      "Accuracy: 0.74 (+/- 0.04) [Ensemble]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'grid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-153-7a0d07d3b9c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f1_micro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: %0.2f (+/- %0.2f) [%s]\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0meclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'grid' is not defined"
     ]
    }
   ],
   "source": [
    "scoring = ['precision_macro', 'recall_macro']\n",
    "estimators = []\n",
    "clf1 = MultinomialNB()\n",
    "estimators.append(clf1)\n",
    "clf2 = svm.SVC(C=100, kernel='linear')\n",
    "estimators.append(clf2)\n",
    "clf3 = svm.SVC(C=100, kernel='rbf', gamma=0.01)\n",
    "estimators.append(clf3)\n",
    "eclf = VotingClassifier(estimators=[('nb', clf1), ('svml', clf2), ('svmr', clf3)], voting='hard')\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['Naive Bayes', 'SVM Linear', 'SVM RBF', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, X, y, cv=5, scoring='f1_micro')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('nb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)), ('svml', SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))],\n",
       "         flatten_transform=None, n_jobs=None, voting='hard', weights=None)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clf = svm.SVC(C=100, kernel='linear')\n",
    "# clf = MultinomialNB()\n",
    "clf = eclf\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[573 478 366 271 837]\n"
     ]
    }
   ],
   "source": [
    "# print(clf.n_support_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2  15   0   0  73   0 246]\n",
      " [  1   5   0   1  62   0 168]\n",
      " [  0   1   0   0  10   0  21]\n",
      " [  0   1   0   0  11   0  23]\n",
      " [  1   8   0   0  45   0 116]\n",
      " [  1   4   0   0  29   0  62]\n",
      " [  7  23   1   0 209   0 511]]\n"
     ]
    }
   ],
   "source": [
    "# cm = confusion_matrix(y_true, y_pred)\n",
    "cm = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "print(cm)\n",
    "cm_np = np.asarray(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 670  439    4   59  354  108 1681]\n"
     ]
    }
   ],
   "source": [
    "TP = np.diag(cm_np)\n",
    "print(TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-658 -382   -3  -58   85 -108 -534]\n"
     ]
    }
   ],
   "source": [
    "FP = np.sum(cm, axis=0) - TP\n",
    "print(FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 60  40  14  10  49  33 120]\n"
     ]
    }
   ],
   "source": [
    "FN = np.sum(cm, axis=1) - TP\n",
    "print(FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[592, 644, 813, 804, 699, 738, 358]\n"
     ]
    }
   ],
   "source": [
    "num_classes = 7\n",
    "TN = []\n",
    "for i in range(num_classes):\n",
    "    temp = np.delete(cm, i, 0)    # delete ith row\n",
    "    temp = np.delete(temp, i, 1)  # delete ith column\n",
    "    TN.append(sum(sum(temp)))\n",
    "print(TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy [0.83956574 0.86489747 0.98069964 0.97949337 0.89626055 0.91073583\n",
      " 0.74185766]\n",
      "precision [0.58757062 0.50344828 0.         0.53333333 0.54320988 0.29310345\n",
      " 0.73219373]\n",
      "recall [0.63414634 0.6460177  0.         0.44444444 0.47311828 0.34\n",
      " 0.68169761]\n",
      "f1 [0.60997067 0.56589147        nan 0.48484848 0.50574713 0.31481481\n",
      " 0.70604396]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in true_divide\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "prec = TP/(TP+FP)\n",
    "rec = TP/(TP+FN)\n",
    "acc = (TP+TN)/(TP+FP+TN+FN)\n",
    "f1 = 2*prec*rec/(prec+rec)\n",
    "\n",
    "print(\"accuracy\", acc)\n",
    "print(\"precision\", prec)\n",
    "print(\"recall\", rec)\n",
    "print(\"f1\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_eval_metrics(x, y, y2, y3, y4, img_name):\n",
    "    fig, ax = plt.subplots()\n",
    "    y_label = \"value\"\n",
    "    x_label = \"classes\"\n",
    "    plt.plot(x, y, color='b', marker='o', label=\"Accuracy\")\n",
    "    plt.plot(x, y2, color='g', marker='+', label=\"Precision\")\n",
    "    plt.plot(x, y3, color='y', marker='x', label=\"Recall\")\n",
    "    plt.plot(x, y4, color='r', marker='s', label=\"F1-measure\")\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.legend()\n",
    "#     plt.scatter(x, y, label=\"Accuracy\")\n",
    "#     plt.scatter(x, y2, label=\"Precision\")\n",
    "#     plt.scatter(x, y3, label=\"Recall\")\n",
    "#     plt.scatter(x, y4, label=\"F1-measure\")\n",
    "    plt.show()\n",
    "    fig.savefig(img_name)\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_eval_metrics(classes, acc, prec, rec, f1, \"tfidf-mNB.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
